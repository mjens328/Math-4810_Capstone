---
title: "**PCA**"
format: 
    html:
        embed-resources: true
urlcolor: blue
engine: jupyter3
execute: 
  eval: true
  echo: true
editor_options: 
  chunk_output_type: console
---

```{python}
#| label: Initial Imports
#| cache: true

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from scipy import stats
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import QuantileTransformer
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans

```
I will be using the Parcel Export Sheet 2 data for the time being as Joseph had stated 
that best case scenario this is the data used to predict with. While he had mentioned 
that some of the data is out of date, I wanted to see what insights could be pulled from 
this first, then work with the other datasets later on as we get a better understanding 
of the data. 

```{python}
#| label: Read in Parcel Export Sheet 2
#| cache: true

parcel_export = pd.read_excel("/Users/melaniejensen/Library/CloudStorage/OneDrive-SouthernUtahUniversity/Math-4810_Capstone/data/Parcel Export.xlsx", sheet_name="Full Parcel Summary")
parcel_export.head()
parcel_export.shape # (53277, 53)
parcel_export.info()

```

```{python}
#| label: Non-numeric columns
#| cache: true

non_numeric_cols = parcel_export.select_dtypes(exclude=[np.number]).columns.tolist()
parcel_export[non_numeric_cols].shape # (53277, 12)
parcel_export[non_numeric_cols].info()
parcel_export[non_numeric_cols].columns

```

```{python}
#| label: Numeric columns
#| cache: true

numeric_cols = parcel_export.select_dtypes(include=[np.number]).columns.tolist() 
parcel_export[numeric_cols].shape # (53277, 41)
parcel_export[numeric_cols].info() 
parcel_export[numeric_cols].columns
```

```{python}
#| label: Numeric Column Adjustment

cols_to_move = ['TaxYear', 'ImpOnly', 'PropType ', 'Jurisdiction', 'SpecificPropType', 'Lat', 'Lng', 'ReviewedDate', 'ImpCode', 'Res Imp Count']
non_numeric_cols = list(set(non_numeric_cols) | set(cols_to_move))
numeric_cols = [c for c in parcel_export.columns if c not in non_numeric_cols]

parcel_export[numeric_cols].shape # (53277, 31)

```

```{python}
#| label: Fill in NAs for numeric columns with 0

parcel_export[numeric_cols] = parcel_export[numeric_cols].fillna(0)

```

```{python}
#| label: Filter Parcel Export for PCA (only numeric columns)
#| cache: true

numeric_parcel_data = parcel_export[numeric_cols]

numeric_parcel_data.shape # (53277, 24)
numeric_parcel_data.info()

```

```{python}
#| label: Perform Transformations

def normalize(df):
    """
    Returns two dataframes:
    1. df_quantile: Everything forced to normal via Quantile Transformation.
    2. df_comp: Each column transformed by the 'winner' of YJ vs. Quantile.
    """
    df_quantile = pd.DataFrame(index=df.index)
    df_comp = pd.DataFrame(index=df.index)
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    for col in numeric_cols:
        data_raw = df[col].values.reshape(-1, 1)
        
        # --- 1. Generate Quantile Transformation (For df_quantile) ---
        qt_engine = QuantileTransformer(output_distribution='normal', 
                                        n_quantiles=min(len(df), 1000), 
                                        random_state=42)
        qt_data = qt_engine.fit_transform(data_raw).flatten()
        df_quantile[f"{col}_quant"] = qt_data
        
        # --- 2. Generate Competition Transformation (For df_comp) ---
        # Yeo-Johnson
        yj_data, lmbda = stats.yeojohnson(df[col])
        _, yj_p = stats.normaltest(yj_data)
        
        # Normality test for the Quantile data we just made
        _, qt_p = stats.normaltest(qt_data)
        
        if yj_p >= qt_p:
            # Determine suffix for YJ winner
            if -0.1 <= lmbda <= 0.1: suffix = "log"
            elif 0.4 <= lmbda <= 0.6: suffix = "sqrt"
            elif 0.9 <= lmbda <= 1.1: suffix = "linear"
            elif lmbda < 0: suffix = "inv"
            else: suffix = "pow"
            df_comp[f"{col}_{suffix}"] = yj_data
        else:
            df_comp[f"{col}_quant"] = qt_data
            
    print(f"Experiments complete for {len(numeric_cols)} variables.")
    return df_quantile, df_comp

df_q_only, df_competition = normalize(numeric_parcel_data)

```

```{python}
#| label: Scaling the data for PCA
#| cache: true

scaler = StandardScaler()
X_scaled_q = scaler.fit_transform(df_q_only)
X_scaled_comp = scaler.fit_transform(df_competition)

```

::: {.panel-tabset}
## Quantile Transformed Data

```{python}
#| label: PCA and Scree Plot
#| cache: true

pca_full = PCA(n_components=None) # n_components=None keeps all features as components
pca_full.fit(X_scaled_q)

```
```{python}
#| label: Scree Plot to optimize number of components
#| cache: true

plt.figure(figsize=(8, 6))
plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1), 
         pca_full.explained_variance_ratio_.cumsum(), 
         marker='o', linestyle='--')
plt.title('Scree Plot (Cumulative Variance)')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.grid()
plt.show()

```
```{python}
#| label: PCA with 13 components
#| cache: true

pca = PCA(n_components=13)
principal_components = pca.fit_transform(X_scaled_q)

pca_df = pd.DataFrame(data=principal_components, columns=[
    'PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13'])

pca.explained_variance_ratio_.sum()

```
```{python}
#| label: TNSE on PCA components
#| cache: true

tsne = TSNE(n_components=2, random_state=42, max_iter=5000)
tsne_results = tsne.fit_transform(pca_df)

plt.figure(figsize=(8,6))
plt.scatter(tsne_results[:,0], tsne_results[:,1], alpha=0.5)
plt.title('T-SNE Visualization of Clusters')
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.grid()
plt.show()

```

```{python}
#| label: Optimize K for K-Means using Elbow Method
#| cache: true

wcss = []
for i in range(1, 11): 
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42, n_init=10)
    kmeans.fit(pca_df)
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(10, 5))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('WCSS')
plt.xticks(range(1, 11))
plt.grid()
plt.show()

```
```{python}
#| label: K-Means Clustering and Visualization for K=2 to K = 6

for i in range(2,6):
    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10) 
    cluster_labels = kmeans.fit_predict(pca_df)

    tsne_df = pd.DataFrame(tsne_results, columns=['tSNE Dim 1', 'tSNE Dim 2'])
    tsne_df['Cluster'] = cluster_labels

    plt.figure(figsize=(10, 8))
    sns.scatterplot(
        data=tsne_df, 
        x='tSNE Dim 1', 
        y='tSNE Dim 2', 
        hue='Cluster', 
        palette=sns.color_palette('Accent', n_colors=6), 
        legend='full',
        alpha=0.7)
    plt.title('T-SNE Visualization Colored by K-Means Clusters')
    plt.xlabel('t-SNE Dimension 1')
    plt.ylabel('t-SNE Dimension 2')
    plt.grid(True)
    plt.show()

```
```{python}
#| label: Relate back to original features for K=2
#| cache: true

kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(pca_df)
df_q_only['Cluster'] = cluster_labels

# Distribution across Clusters
cluster_counts = df_q_only['Cluster'].value_counts()
print("Cluster Counts:")
print(cluster_counts)

# Calculate and compare the mean of all features for each cluster
cluster_profiles = df_q_only.groupby('Cluster').mean().T
print(cluster_profiles)

```

## Comp Transformed Data

```{python}
#| label: PCA and Scree Plot p-two
#| cache: true

pca_full = PCA(n_components=None) # n_components=None keeps all features as components
pca_full.fit(X_scaled_comp)

```
```{python}
#| label: Scree Plot to optimize number of components p-two
#| cache: true

plt.figure(figsize=(8, 6))
plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1), 
         pca_full.explained_variance_ratio_.cumsum(), 
         marker='o', linestyle='--')
plt.title('Scree Plot (Cumulative Variance)')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.grid()
plt.show()

```
```{python}
#| label: PCA with 12 components p-two
#| cache: true

pca = PCA(n_components=12)
principal_components = pca.fit_transform(X_scaled_comp)

pca_df = pd.DataFrame(data=principal_components, columns=[
    'PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12'])

pca.explained_variance_ratio_.sum()

```
```{python}
#| label: TNSE on PCA components p-two
#| cache: true

tsne = TSNE(n_components=2, random_state=42, max_iter=5000)
tsne_results = tsne.fit_transform(pca_df)

plt.figure(figsize=(8,6))
plt.scatter(tsne_results[:,0], tsne_results[:,1], alpha=0.5)
plt.title('T-SNE Visualization of Clusters')
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.grid()
plt.show()

```

```{python}
#| label: Optimize K for K-Means using Elbow Method p-two
#| cache: true

wcss = []
for i in range(1, 11): 
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42, n_init=10)
    kmeans.fit(pca_df)
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(10, 5))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('WCSS')
plt.xticks(range(1, 11))
plt.grid()
plt.show()

```
```{python}
#| label: K-Means Clustering and Visualization for K=2 to K=5 p-two
#| cache: true

for i in range(2,6):
    kmeans = KMeans(n_clusters=i, random_state=42, n_init=10) 
    cluster_labels = kmeans.fit_predict(pca_df)

    tsne_df = pd.DataFrame(tsne_results, columns=['tSNE Dim 1', 'tSNE Dim 2'])
    tsne_df['Cluster'] = cluster_labels

    plt.figure(figsize=(10, 8))
    sns.scatterplot(
        data=tsne_df, 
        x='tSNE Dim 1', 
        y='tSNE Dim 2', 
        hue='Cluster', 
        palette=sns.color_palette('Accent', n_colors=6), 
        legend='full',
        alpha=0.7)
    plt.title('T-SNE Visualization Colored by K-Means Clusters')
    plt.xlabel('t-SNE Dimension 1')
    plt.ylabel('t-SNE Dimension 2')
    plt.grid(True)
    plt.show()

```
```{python}
#| label: Relate back to original features for K=2 p-two
#| cache: true

kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(pca_df)
df_competition['Cluster'] = cluster_labels

# Distribution across Clusters
cluster_counts = df_competition['Cluster'].value_counts()
print("Cluster Counts:")
print(cluster_counts)

# Calculate and compare the mean of all features for each cluster
cluster_profiles = df_competition.groupby('Cluster').mean().T
print(cluster_profiles)

```

:::